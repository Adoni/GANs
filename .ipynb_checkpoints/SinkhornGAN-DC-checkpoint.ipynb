{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy\n",
    "import os\n",
    "if 'DISPLAY' not in os.environ:\n",
    "    matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import utils\n",
    "show_image=True\n",
    "def imshow(inp, file_name, save=False, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp[inp>1]=1\n",
    "    inp[inp<-1]=-1\n",
    "    plt.imshow(inp)\n",
    "    plt.savefig(file_name)\n",
    "    if show_image:\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Don't show\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_size=128\n",
    "hidden_size=64\n",
    "img_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = './data'\n",
    "download = True\n",
    "trans = transforms.Compose([\n",
    "    transforms.Scale(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "train_set = dset.MNIST(\n",
    "    root=root, train=True, transform=trans, download=download)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=trans)\n",
    "batch_size = 128\n",
    "kwargs = {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_size,hidden_size*8,4,2,1),\n",
    "            nn.BatchNorm2d(hidden_size*8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(hidden_size*8,hidden_size*4,4,2,1),\n",
    "            nn.BatchNorm2d(hidden_size*4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(hidden_size*4,hidden_size*2,4,2,1),\n",
    "            nn.BatchNorm2d(hidden_size*2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(hidden_size*2,hidden_size,4,2,1),\n",
    "            nn.BatchNorm2d(hidden_size*1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(hidden_size,1,4,2,1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], z_size, 1, 1)\n",
    "        out = self.model(x)\n",
    "        out = out.view(x.size()[0], 1, img_size,img_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), img_size * img_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one = torch.FloatTensor([1])\n",
    "noise_holder=torch.FloatTensor(batch_size, z_size, 1, 1)\n",
    "input_holder = torch.FloatTensor(batch_size, 1, img_size, img_size)\n",
    "if torch.cuda.is_available():\n",
    "    one=one.cuda()\n",
    "    noise_holder=noise_holder.cuda()\n",
    "    input_holder=input_holder.cuda()\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator (\n",
      "  (model): Sequential (\n",
      "    (0): ConvTranspose2d(128, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU (inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): ReLU (inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (8): ReLU (inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (11): ReLU (inplace)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (13): Tanh ()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception in thread Thread-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sunxiaofei/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/sunxiaofei/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/Users/sunxiaofei/anaconda3/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "G = Generator()\n",
    "G.apply(weights_init)\n",
    "D = Discriminator()\n",
    "print(G)\n",
    "if torch.cuda.is_available():\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "G_lr = D_lr = 5e-5\n",
    "optimizers = {\n",
    "    'D': torch.optim.RMSprop(D.parameters(), lr=D_lr),\n",
    "    'G': torch.optim.RMSprop(G.parameters(), lr=G_lr)\n",
    "}\n",
    "criterion = nn.BCELoss()\n",
    "for epoch in tqdm(range(10000)):\n",
    "    for p in D.parameters():\n",
    "        p.requires_grad = True\n",
    "    if epoch<25 or epoch%100==0:\n",
    "        iter_D=100\n",
    "    else:\n",
    "        iter_D=10\n",
    "    for _ in range(iter_D):\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "        optimizers['D'].zero_grad()\n",
    "        data=next(iter(train_loader))[0]\n",
    "        if torch.cuda.is_available():\n",
    "            data=data.cuda()\n",
    "        input_holder.resize_as_(data).copy_(data)\n",
    "        y = D(Variable(data))\n",
    "        noise_holder.resize_(data.size()[0], z_size, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise_holder,volatile=True)\n",
    "        fake_data = Variable(G(noisev).data)\n",
    "        x = D(fake_data)\n",
    "        W_xy=sinkhorn_loss(x,y,epsilon,n,niter)\n",
    "        W_xx=sinkhorn_loss(x,x,epsilon,n,niter)\n",
    "        W_yy=sinkhorn_loss(y,y,epsilon,n,niter)\n",
    "        Loss_D=2*W_xy-W_xx-W_yy\n",
    "        Loss_D.backward()\n",
    "        optimizers['D'].step()\n",
    "\n",
    "    for p in D.parameters():\n",
    "        p.requires_grad = False\n",
    "    optimizers['G'].zero_grad()\n",
    "    noise_holder.resize_(data.size()[0], z_size, 1, 1).normal_(0, 1)\n",
    "    noisev = Variable(noise_holder)\n",
    "    fake_data = G(noisev)\n",
    "    output_fake1 = D(fake_data)\n",
    "    output_fake1.backward(one)\n",
    "#     G_loss = -torch.mean(output_fake1)\n",
    "\n",
    "#     G_loss.backward()\n",
    "    optimizers['G'].step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        if torch.cuda.is_available():\n",
    "            dd = utils.make_grid(fake_data.cpu().data[:64])\n",
    "        else:\n",
    "            dd = utils.make_grid(fake_data.data[:64])\n",
    "        imshow(dd,'./results/WGAN_DC_%d.png'%(epoch))\n",
    "        dd = dd.mul(0.5).add(0.5)\n",
    "        vutils.save_image(dd, './results/OrWGAN_DC_%d.png'%(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
