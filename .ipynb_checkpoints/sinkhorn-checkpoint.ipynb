{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from IPython.core.debugger import Tracer\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, save=False, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    inp = transforms.ToPILImage()(inp)\n",
    "    plt.imshow(inp,cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_G(nn.Module):\n",
    "    def __init__(self, isize, nz, nc, ngf, ngpu):\n",
    "        super(MLP_G, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            # Z goes into a linear of size: ngf\n",
    "            nn.Linear(nz, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ngf, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ngf, ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ngf, nc * isize * isize),\n",
    "        )\n",
    "        self.main = main\n",
    "        self.nc = nc\n",
    "        self.isize = isize\n",
    "        self.nz = nz\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0), input.size(1))\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output.view(output.size(0), self.nc, self.isize, self.isize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_D(nn.Module):\n",
    "    def __init__(self, isize, nz, nc, ndf, ngpu):\n",
    "        super(MLP_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "\n",
    "        main = nn.Sequential(\n",
    "            # Z goes into a linear of size: ndf\n",
    "            nn.Linear(nc * isize * isize, ndf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ndf, ndf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ndf, ndf),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(ndf, 1),\n",
    "        )\n",
    "        self.main = main\n",
    "        self.nc = nc\n",
    "        self.isize = isize\n",
    "        self.nz = nz\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0),\n",
    "                           input.size(1) * input.size(2) * input.size(3))\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        output = output.mean(0)\n",
    "        return output.view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN_D(nn.Module):\n",
    "    def __init__(self, isize, nz, nc, ndf, ngpu, n_extra_layers=0):\n",
    "        super(DCGAN_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc, ndf),\n",
    "                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        csize, cndf = isize / 2, ndf\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cndf),\n",
    "                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cndf),\n",
    "                            nn.BatchNorm2d(cndf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cndf),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "\n",
    "        while csize > 4:\n",
    "            in_feat = cndf\n",
    "            out_feat = cndf * 2\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "            cndf = cndf * 2\n",
    "            csize = csize / 2\n",
    "\n",
    "        # state size. K x 4 x 4\n",
    "        main.add_module('final.{0}-{1}.conv'.format(cndf, 1),\n",
    "                        nn.Conv2d(cndf, 1, 4, 1, 0, bias=False))\n",
    "        self.main = main\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "            \n",
    "        output = output.mean(0)\n",
    "        return output.view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN_G(nn.Module):\n",
    "    def __init__(self, isize, nz, nc, ngf, ngpu, n_extra_layers=0):\n",
    "        super(DCGAN_G, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "\n",
    "        cngf, tisize = ngf//2, 4\n",
    "        while tisize != isize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "\n",
    "        main = nn.Sequential()\n",
    "        # input is Z, going into a convolution\n",
    "        main.add_module('initial.{0}-{1}.convt'.format(nz, cngf),\n",
    "                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n",
    "        main.add_module('initial.{0}.batchnorm'.format(cngf),\n",
    "                        nn.BatchNorm2d(cngf))\n",
    "        main.add_module('initial.{0}.relu'.format(cngf),\n",
    "                        nn.ReLU(True))\n",
    "\n",
    "        csize, cndf = 4, cngf\n",
    "        while csize < isize//2:\n",
    "            main.add_module('pyramid.{0}-{1}.convt'.format(cngf, cngf//2),\n",
    "                            nn.ConvTranspose2d(cngf, cngf//2, 4, 2, 1, bias=False))\n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(cngf//2),\n",
    "                            nn.BatchNorm2d(cngf//2))\n",
    "            main.add_module('pyramid.{0}.relu'.format(cngf//2),\n",
    "                            nn.ReLU(True))\n",
    "            cngf = cngf // 2\n",
    "            csize = csize * 2\n",
    "\n",
    "        # Extra layers\n",
    "        for t in range(n_extra_layers):\n",
    "            main.add_module('extra-layers-{0}.{1}.conv'.format(t, cngf),\n",
    "                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n",
    "            main.add_module('extra-layers-{0}.{1}.batchnorm'.format(t, cngf),\n",
    "                            nn.BatchNorm2d(cngf))\n",
    "            main.add_module('extra-layers-{0}.{1}.relu'.format(t, cngf),\n",
    "                            nn.ReLU(True))\n",
    "\n",
    "        main.add_module('final.{0}-{1}.convt'.format(cngf, nc),\n",
    "                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n",
    "        main.add_module('final.{0}.tanh'.format(nc),\n",
    "                        nn.Tanh())\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else: \n",
    "            output = self.main(input)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_matrix(x, y,p=2) :\n",
    "    \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
    "    x_col = x.unsqueeze(1) \n",
    "    y_lin = y.unsqueeze(0)\n",
    "    c = torch.sum( (torch.abs(x_col - y_lin))**p , 2) \n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_loss(x,y,epsilon,n,niter) :\n",
    "    \n",
    "    \"\"\"\n",
    "    Given two emprical measures with n points each with locations x and y \n",
    "    outputs an approximation of the OT cost with regularization parameter epsilon\n",
    "    niter is the max. number of steps in sinkhorn loop\n",
    "    \"\"\"\n",
    "    # The Sinkhorn algorithm takes as input three variables :\n",
    "    C = cost_matrix(x, y) # Wasserstein cost function\n",
    "    # both marginals are fixed with equal weights\n",
    "    if torch.cuda.is_available():\n",
    "        mu = Variable(1./n*torch.cuda.FloatTensor(n).fill_(1),requires_grad=False) \n",
    "        nu = Variable(1./n*torch.cuda.FloatTensor(n).fill_(1),requires_grad=False)\n",
    "    else:\n",
    "        mu = Variable(1./n*torch.FloatTensor(n).fill_(1),requires_grad=False) \n",
    "        nu = Variable(1./n*torch.FloatTensor(n).fill_(1),requires_grad=False)\n",
    "    \n",
    "    # Parameters of the Sinkhorn algorithm.\n",
    "    rho                = 1 #(.5) **2          # unbalanced transport \n",
    "    tau                = -.8               # nesterov-like acceleration\n",
    "    lam = rho / (rho + epsilon)            # Update exponent\n",
    "    thresh = 10**(-1)                   # stopping criterion\n",
    "\n",
    "    # Elementary operations .....................................................................\n",
    "    def ave(u,u1) : \n",
    "        \"Barycenter subroutine, used by kinetic acceleration through extrapolation.\"\n",
    "        return tau * u + (1-tau) * u1 \n",
    "\n",
    "    def M(u,v)  : \n",
    "        \"Modified cost for logarithmic updates\"\n",
    "        \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n",
    "        return (-C + u.unsqueeze(1) + v.unsqueeze(0)) / epsilon\n",
    "\n",
    "    def lse(A) :\n",
    "        \"log-sum-exp\" \n",
    "        return torch.log(torch.exp(A).sum( 1, keepdim = True ) + 1e-6) # add 10^-6 to prevent NaN\n",
    "    \n",
    "    # Actual Sinkhorn loop ......................................................................\n",
    "    u,v,err = epsilon * torch.log(mu), epsilon * torch.log(nu), 0.\n",
    "    actual_nits = 0 # to check if algorithm terminates because of threshold or max iterations reached\n",
    "    \n",
    "    for i in range(niter) :\n",
    "        u1 = u # useful to check the update\n",
    "        #Tracer()()\n",
    "        u =  epsilon * ( torch.log(mu) - lse(M(u,v)).squeeze() ) + u\n",
    "        v =  epsilon * ( torch.log(nu) - lse(M(u,v).t()).squeeze()) + v\n",
    "        #u = epsilon * (torch.log(mu) - (-C/epsilon + epsilon * ) / epsilon + 1e-6 ))\n",
    "        # accelerated unbalanced iterations \n",
    "        #u = ave( u, lam * ( epsilon * ( torch.log(mu.unsqueeze(1)) - lse(M(u,v))   ) + u ) )\n",
    "        #v = ave( v, lam * ( epsilon * ( torch.log(nu.unsqueeze(1)) - lse(M(u,v).t()) ) + v ) )\n",
    "        err = (u - u1).abs().sum()\n",
    "        #actual_nits += 1\n",
    "        if (err < thresh).data.cpu().numpy() :\n",
    "            break\n",
    "    U, V = u, v \n",
    "    #Tracer()()\n",
    "    logC = torch.log(C + 1e-5)\n",
    "    logpi = M(U,V)\n",
    "    #pi = torch.exp(  ) # Transport plan pi = diag(a)*K*diag(b)\n",
    "    cost  = torch.sum(torch.exp(logC + logpi))        # Sinkhorn cost\n",
    "     \n",
    "    return cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_normalized(x,y,epsilon,n,niter):\n",
    "\n",
    "    x = x.view(x.size(0), x.size(1) * x.size(2) * x.size(3))\n",
    "    y = y.view(y.size(0), y.size(1) * y.size(2) * y.size(3))\n",
    "    Wxy = sinkhorn_loss(x,y,epsilon,n,niter)\n",
    "    Wxx = sinkhorn_loss(x,x,epsilon,n,niter)\n",
    "    Wyy = sinkhorn_loss(y,y,epsilon,n,niter)\n",
    "    return 2*Wxy - Wxx - Wyy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpu = 1 # number of GPUs to use\n",
    "nz = 100 # size of the latent z vector\n",
    "ngf = 512\n",
    "ndf = 512\n",
    "nc = 1 # input image channels\n",
    "n_extra_layers = 0 # Number of extra layers on gen and disc\n",
    "\n",
    "imageSize = 28\n",
    "batchSize = 200\n",
    "n_workers = 2\n",
    "\n",
    "\n",
    "adam = False\n",
    "lrG = 0.0005\n",
    "\n",
    "beta1 = 0.5 # beta1 for adam. default=0.5\n",
    "niter = 2 # number of epochs to train for\n",
    "\n",
    "clamp_lower = -0.1\n",
    "clamp_upper = 0.1\n",
    "\n",
    "experiment = './experiment' # Where to store samples and models\n",
    "\n",
    "epsilon = 1 # panalty weight\n",
    "L = 10 # sinkhorn iteration num\n",
    "\n",
    "netG_path = experiment + '/netG_sinkhorn.pth'\n",
    "netD_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "dataset = dset.MNIST(root='./data', download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Scale(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize,\n",
    "                                         shuffle=True, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load parameters\n",
      "MLP_G (\n",
      "  (main): Sequential (\n",
      "    (0): Linear (100 -> 512)\n",
      "    (1): ReLU (inplace)\n",
      "    (2): Linear (512 -> 512)\n",
      "    (3): ReLU (inplace)\n",
      "    (4): Linear (512 -> 512)\n",
      "    (5): ReLU (inplace)\n",
      "    (6): Linear (512 -> 784)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = MLP_G(imageSize, nz, nc, ngf, ngpu)\n",
    "\n",
    "\n",
    "netG.apply(weights_init)\n",
    "try: # load checkpoint if needed\n",
    "    netG.load_state_dict(torch.load(netG_path))\n",
    "    print(\"load parameters\")\n",
    "except:\n",
    "    pass\n",
    "print(netG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.FloatTensor(batchSize, 3, imageSize, imageSize)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'netD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-20e89d28f37b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'netD' is not defined"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    netG.cuda()\n",
    "    input = input.cuda()\n",
    "    one, mone = one.cuda(), mone.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if adam:\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lrG, betas=(beta1, 0.999))\n",
    "else:\n",
    "    optimizerG = optim.RMSprop(netG.parameters(), lr = lrG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "gen_iterations = 0\n",
    "for epoch in range(niter):\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    i = 0\n",
    "    while i < len(dataloader):\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        data = data_iter.next()\n",
    "        real_cpu, _ = data\n",
    "        if torch.cuda.is_available():\n",
    "            real_cpu = real_cpu.cuda()\n",
    "        netG.zero_grad()\n",
    "        # in case our last batch was the tail batch of the dataloader,\n",
    "        # make sure we feed a full batch of noise\n",
    "        noise.resize_(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "        if torch.cuda.is_available():\n",
    "            noise = noise.cuda()\n",
    "        noisev = Variable(noise, requires_grad = False)\n",
    "        fake = netG(noisev)\n",
    "        #Tracer()()\n",
    "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "        inputv = Variable(input, requires_grad = False)\n",
    "        loss = sinkhorn_normalized(fake, inputv, epsilon, batchSize, L) #- compute_sinkhorn_loss(batchSize, fake, fake, epsilon, L) - compute_sinkhorn_loss(batchSize, inputv, inputv, epsilon, L)\n",
    "        loss.backward(one)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        print('[%d/%d][%d/%d][%d] Sinkhorn_Loss: %f'\n",
    "            % (epoch, niter, i, len(dataloader), gen_iterations,\n",
    "            loss.data[0]))\n",
    "        if gen_iterations % 50 == 0:\n",
    "            real_cpu = real_cpu.mul(0.5).add(0.5)\n",
    "            vutils.save_image(real_cpu, '{0}/real_samples_sinkhorn.png'.format(experiment))\n",
    "            fake = netG(Variable(fixed_noise, volatile=True))\n",
    "            fake.data = fake.data.mul(0.5).add(0.5)\n",
    "            vutils.save_image(fake.data, '{0}/fake_samples_sinkhorn.png'.format(experiment))\n",
    "            if torch.cuda.is_available():\n",
    "                dd = utils.make_grid(fake.cpu().data[:16])\n",
    "            else:\n",
    "                dd = utils.make_grid(fake.data[:16]) \n",
    "            imshow(dd)\n",
    "            # do checkpointing\n",
    "            torch.save(netG.state_dict(), '{0}/netG_sinkhorn.pth'.format(experiment))\n",
    "        i += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
